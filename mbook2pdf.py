#!/usr/bin/env python3
"""
é€šç”¨ mdBook ç½‘ç«™çˆ¬è™« - è‡ªåŠ¨çˆ¬å–å¹¶ç”Ÿæˆ PDF

æ”¯æŒæ‰€æœ‰ mdBook æ„å»ºçš„æ–‡æ¡£ç«™ç‚¹ï¼Œå¦‚:
- https://rustwiki.org/zh-CN/book/
- https://rustwiki.org/zh-CN/rust-by-example/
- https://colobu.com/rust100/
- ä»¥åŠå…¶ä»– mdBook ç«™ç‚¹

ä½¿ç”¨æ–¹æ³•:
    pip install requests beautifulsoup4 weasyprint
    python mbook2pdf.py <URL>

ç¤ºä¾‹:
    python mbook2pdf.py https://rustwiki.org/zh-CN/book/
    python mbook2pdf.py https://colobu.com/rust100/

macOS é¢å¤–ä¾èµ–:
    brew install pango

Ubuntu é¢å¤–ä¾èµ–:
    sudo apt install libpango-1.0-0 libpangocairo-1.0-0
"""

import argparse
import os
import re
import sys
import time
from collections import OrderedDict
from typing import Dict, List, Optional
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

# å¸¸é‡å®šä¹‰
DEFAULT_DELAY = 0.3
DEFAULT_TIMEOUT = 30
DEFAULT_REQUEST_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
}

# CSS é€‰æ‹©å™¨é…ç½®
SIDEBAR_SELECTORS = [
    ('nav', {'class_': 'sidebar'}),
    ('div', {'class_': 'sidebar'}),
    ('div', {'id': 'sidebar'}),
    ('nav', {'class_': 'nav-chapters'}),
    ('ol', {'class_': 'chapter'}),
    ('ul', {'class_': 'chapter'}),
]

MAIN_CONTENT_SELECTORS = [
    ('main', {}),
    ('div', {'class_': 'content'}),
    ('div', {'id': 'content'}),
    ('article', {}),
    ('div', {'class_': 'page-wrapper'}),
]

# éœ€è¦ç§»é™¤çš„å…ƒç´ é…ç½®
REMOVE_TAGS = ['nav', 'header', 'footer', 'script', 'style', 'noscript']

REMOVE_CLASSES = [
    'nav-wrapper', 'nav-chapters', 'sidebar', 'menu-bar',
    'nav-wide-wrapper', 'sidetoc', 'pagetoc', 'mobile-nav-chapters',
    'buttons', 'search-wrapper', 'searchresults-outer', 'searchresults-header',
    'theme-popup', 'theme-toggle', 'search-toggle', 'print-button',
    'git-link', 'edit-button', 'back-to-top', 'chapter-nav'
]

REMOVE_IDS = [
    'sidebar', 'menu-bar', 'search-wrapper', 'searchresults-outer',
    'theme-toggle', 'search-toggle', 'searchbar', 'searchresults'
]

# æ–‡ä»¶åå®‰å…¨å­—ç¬¦æ­£åˆ™
FILENAME_UNSAFE_CHARS = r'[<>:"/\\|?*]'


class MdBookCrawler:
    """mdBook ç½‘ç«™çˆ¬è™«ç±»ï¼Œç”¨äºçˆ¬å–å¹¶ç”Ÿæˆ PDF"""
    
    def __init__(self, base_url: str, output_dir: Optional[str] = None, delay: float = DEFAULT_DELAY):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            base_url: mdBook ç½‘ç«™çš„åŸºç¡€ URL
            output_dir: è¾“å‡ºç›®å½•ï¼Œå¦‚æœä¸º None åˆ™è‡ªåŠ¨ç”Ÿæˆ
            delay: è¯·æ±‚é—´éš”ç§’æ•°
        """
        # ç¡®ä¿ URL ä»¥ / ç»“å°¾
        self.base_url = base_url if base_url.endswith('/') else base_url + '/'
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update(DEFAULT_REQUEST_HEADERS)
        
        # ä» URL æå–ç«™ç‚¹åç§°ä½œä¸ºè¾“å‡ºç›®å½•
        self.output_dir = output_dir or self._generate_output_dir(base_url)
        self.book_title: Optional[str] = None
        self.chapters: OrderedDict[str, str] = OrderedDict()
        self.pages: List[Dict[str, str]] = []
    
    @staticmethod
    def _generate_output_dir(base_url: str) -> str:
        """ä» URL ç”Ÿæˆè¾“å‡ºç›®å½•å"""
        parsed = urlparse(base_url)
        path_parts = [p for p in parsed.path.split('/') if p]
        site_name = path_parts[-1] if path_parts else parsed.netloc.replace('.', '_')
        return f"./{site_name}_pdf"
    
    def fetch_page(self, url: str) -> Optional[str]:
        """
        è·å–é¡µé¢å†…å®¹
        
        Args:
            url: è¦è·å–çš„é¡µé¢ URL
            
        Returns:
            é¡µé¢ HTML å†…å®¹ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            response = self.session.get(url, timeout=DEFAULT_TIMEOUT)
            response.encoding = 'utf-8'
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            print(f"\n  âš ï¸  è·å–å¤±è´¥ {url}: {e}")
            return None
    
    def _extract_book_title(self, soup: BeautifulSoup) -> str:
        """ä» HTML ä¸­æå–ä¹¦ç±æ ‡é¢˜"""
        # æ–¹æ³•1: ä»èœå•æ ‡é¢˜æˆ–ä¾§è¾¹æ  logo è·å–
        title_elem = soup.find('h1', class_='menu-title') or soup.find('a', class_='sidebar-logo')
        if title_elem:
            title = title_elem.get_text(strip=True)
            if title:
                return title
        
        # æ–¹æ³•2: ä»é¡µé¢ title æ ‡ç­¾è·å–
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.get_text(strip=True).split(' - ')[0].strip()
            if title:
                return title
        
        # é»˜è®¤æ ‡é¢˜
        return "mdBook æ–‡æ¡£"
    
    def _find_sidebar(self, soup: BeautifulSoup):
        """æŸ¥æ‰¾ä¾§è¾¹æ å…ƒç´ """
        for tag, attrs in SIDEBAR_SELECTORS:
            sidebar = soup.find(tag, **attrs)
            if sidebar:
                return sidebar
        return None
    
    def _is_valid_chapter_link(self, href: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ç« èŠ‚é“¾æ¥"""
        # è·³è¿‡é”šç‚¹é“¾æ¥
        if href.startswith('#'):
            return False
        
        # è·³è¿‡å¤–éƒ¨é“¾æ¥
        if href.startswith('http') and self.base_url not in href:
            return False
        
        # è·³è¿‡é HTML é“¾æ¥ï¼ˆæœ‰æ‰©å±•åçš„æ–‡ä»¶ï¼‰
        if href and not href.endswith('.html') and not href.endswith('/'):
            last_part = href.split('/')[-1]
            if '.' in last_part:
                return False
        
        return True
    
    def _normalize_url(self, href: str) -> str:
        """è§„èŒƒåŒ– URL"""
        if href.startswith('http'):
            full_url = href
        else:
            full_url = urljoin(self.base_url, href)
        
        # ç§»é™¤é”šç‚¹
        full_url = full_url.split('#')[0]
        return full_url
    
    def _extract_chapters_from_sidebar(self, sidebar, soup: BeautifulSoup) -> OrderedDict[str, str]:
        """ä»ä¾§è¾¹æ æå–ç« èŠ‚é“¾æ¥"""
        chapters = OrderedDict()
        
        for a in sidebar.find_all('a', href=True):
            href = a.get('href', '')
            
            if not self._is_valid_chapter_link(href):
                continue
            
            title = a.get_text(strip=True)
            if not title:
                continue
            
            full_url = self._normalize_url(href)
            
            # ç¡®ä¿ URL å±äºåŒä¸€ç«™ç‚¹
            if urlparse(full_url).netloc == urlparse(self.base_url).netloc:
                if full_url not in chapters:
                    chapters[full_url] = title
        
        return chapters
    
    def _extract_chapters_from_links(self, soup: BeautifulSoup) -> OrderedDict[str, str]:
        """ä»é¡µé¢æ‰€æœ‰é“¾æ¥ä¸­æå–ç« èŠ‚ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        chapters = OrderedDict()
        
        for a in soup.find_all('a', href=True):
            href = a.get('href', '')
            if href.endswith('.html') and not href.startswith('http'):
                full_url = self._normalize_url(href)
                title = a.get_text(strip=True)
                if title and full_url not in chapters:
                    chapters[full_url] = title
        
        return chapters
    
    def parse_sidebar(self, html: str) -> OrderedDict[str, str]:
        """
        è§£æä¾§è¾¹æ è·å–æ‰€æœ‰ç« èŠ‚é“¾æ¥
        
        Args:
            html: é¦–é¡µ HTML å†…å®¹
            
        Returns:
            ç« èŠ‚å­—å…¸ï¼Œkey ä¸º URLï¼Œvalue ä¸ºæ ‡é¢˜
        """
        soup = BeautifulSoup(html, 'html.parser')
        
        # æå–ä¹¦ç±æ ‡é¢˜
        self.book_title = self._extract_book_title(soup)
        
        chapters = OrderedDict()
        
        # æ–¹æ³•1: ä»ä¾§è¾¹æ æå–
        sidebar = self._find_sidebar(soup)
        if sidebar:
            chapters = self._extract_chapters_from_sidebar(sidebar, soup)
        
        # æ–¹æ³•2: å¦‚æœä¾§è¾¹æ è§£æå¤±è´¥ï¼Œä»é¡µé¢é“¾æ¥ä¸­æå–
        if not chapters:
            chapters = self._extract_chapters_from_links(soup)
        
        # ç¡®ä¿é¦–é¡µåœ¨åˆ—è¡¨ä¸­
        if self.base_url not in chapters:
            chapters[self.base_url] = self.book_title
            chapters.move_to_end(self.base_url, last=False)
        
        return chapters
    
    def _find_main_content(self, soup: BeautifulSoup):
        """æŸ¥æ‰¾ä¸»å†…å®¹åŒºåŸŸ"""
        for tag, attrs in MAIN_CONTENT_SELECTORS:
            main = soup.find(tag, **attrs)
            if main:
                return main
        return soup.find('body')
    
    def _remove_unwanted_elements(self, main):
        """ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ """
        # ç§»é™¤æŒ‡å®šæ ‡ç­¾
        for tag in REMOVE_TAGS:
            for elem in main.find_all(tag):
                elem.decompose()
        
        # ç§»é™¤æŒ‡å®š class çš„å…ƒç´ 
        for class_name in REMOVE_CLASSES:
            for elem in main.find_all(class_=class_name):
                elem.decompose()
        
        # ç§»é™¤æŒ‡å®š id çš„å…ƒç´ 
        for id_name in REMOVE_IDS:
            elem = main.find(id=id_name)
            if elem:
                elem.decompose()
        
        # ç§»é™¤ play æŒ‰é’®ç­‰äº¤äº’å…ƒç´ 
        for elem in main.find_all(['button', 'i'], class_=lambda x: x and (
            'play' in x.lower() or 'copy' in x.lower() or 'fa-' in x
        )):
            elem.decompose()
    
    def _process_headings(self, main):
        """å¤„ç†æ ‡é¢˜ï¼šé™çº§å¹¶ç¦ç”¨ä¹¦ç­¾"""
        # ç§»é™¤é¡µé¢åŸæœ‰çš„ç¬¬ä¸€ä¸ª h1 æ ‡é¢˜ï¼ˆæˆ‘ä»¬ä¼šåœ¨å¤–å±‚æ·»åŠ ç« èŠ‚æ ‡é¢˜ï¼‰
        first_h1 = main.find('h1')
        if first_h1:
            first_h1.decompose()
        
        # å°†å†…å®¹ä¸­çš„æ ‡é¢˜é™çº§ï¼Œé¿å…ä¸ç« èŠ‚æ ‡é¢˜å†²çªï¼ŒåŒæ—¶ç¦ç”¨å®ƒä»¬çš„ä¹¦ç­¾
        # h1 -> h2, h2 -> h3, h3 -> h4 ç­‰
        for i in range(5, 0, -1):  # ä» h5 åˆ° h1
            for h in main.find_all(f'h{i}'):
                h.name = f'h{min(i+1, 6)}'
                # æ·»åŠ  class ç¦ç”¨ä¹¦ç­¾
                existing_classes = h.get('class', [])
                if isinstance(existing_classes, str):
                    existing_classes = [existing_classes]
                h['class'] = existing_classes + ['no-bookmark']
    
    def _fix_media_urls(self, main):
        """ä¿®å¤åª’ä½“èµ„æº URL"""
        # ä¿®å¤å›¾ç‰‡è·¯å¾„
        for img in main.find_all('img'):
            src = img.get('src', '')
            if src and not src.startswith(('http', 'data:')):
                img['src'] = urljoin(self.base_url, src)
        
        # ä¿®å¤é“¾æ¥
        for a in main.find_all('a'):
            href = a.get('href', '')
            if href and not href.startswith(('http', '#', 'mailto:', 'javascript:')):
                a['href'] = urljoin(self.base_url, href)
    
    def extract_content(self, html: str) -> str:
        """
        æå–é¡µé¢ä¸»è¦å†…å®¹
        
        Args:
            html: é¡µé¢ HTML å†…å®¹
            
        Returns:
            æå–åçš„ HTML å†…å®¹
        """
        soup = BeautifulSoup(html, 'html.parser')
        main = self._find_main_content(soup)
        
        if not main:
            return ""
        
        # æ¸…ç†å†…å®¹
        self._remove_unwanted_elements(main)
        self._process_headings(main)
        self._fix_media_urls(main)
        
        return str(main)
    
    def _display_progress(self, current: int, total: int, title: str):
        """æ˜¾ç¤ºçˆ¬å–è¿›åº¦"""
        progress_bar_length = 30
        filled = current * progress_bar_length // total
        progress = "â–ˆ" * filled + "â–‘" * (progress_bar_length - filled)
        display_title = title[:35] if len(title) <= 35 else title[:32] + "..."
        print(f"\r  [{progress}] {current}/{total} {display_title:<35}", end="", flush=True)
    
    def crawl(self) -> bool:
        """
        çˆ¬å–æ‰€æœ‰é¡µé¢
        
        Returns:
            æˆåŠŸè¿”å› Trueï¼Œå¤±è´¥è¿”å› False
        """
        print(f"\nğŸ“– æ­£åœ¨è·å–é¦–é¡µ: {self.base_url}")
        
        index_html = self.fetch_page(self.base_url)
        if not index_html:
            print("âŒ æ— æ³•è·å–é¦–é¡µ")
            return False
        
        print("ğŸ“‹ æ­£åœ¨è§£æç›®å½•ç»“æ„...")
        self.chapters = self.parse_sidebar(index_html)
        
        if not self.chapters:
            print("âŒ æ— æ³•è§£æç›®å½•ç»“æ„")
            return False
        
        print(f"âœ… æ‰¾åˆ° {len(self.chapters)} ä¸ªé¡µé¢")
        print(f"ğŸ“š ä¹¦ç±æ ‡é¢˜: {self.book_title}\n")
        
        pages = []
        total = len(self.chapters)
        
        for i, (url, title) in enumerate(self.chapters.items(), 1):
            self._display_progress(i, total, title)
            
            html = self.fetch_page(url)
            if html:
                content = self.extract_content(html)
                pages.append({
                    'url': url,
                    'title': title,
                    'content': content
                })
            
            time.sleep(self.delay)
        
        print()  # æ¢è¡Œ
        print(f"\nâœ… æˆåŠŸçˆ¬å– {len(pages)} ä¸ªé¡µé¢")
        
        self.pages = pages
        return True
    
    @staticmethod
    def _get_toc_level(title: str) -> int:
        """
        æ ¹æ®æ ‡é¢˜åˆ¤æ–­ç›®å½•å±‚çº§
        
        Args:
            title: ç« èŠ‚æ ‡é¢˜
            
        Returns:
            ç›®å½•å±‚çº§ (1-3)
        """
        # æ£€æŸ¥æ˜¯å¦ä»¥æ•°å­—å¼€å¤´ï¼ˆå¦‚ "1. å…¥é—¨" "1.1 å®‰è£…" "1.1.1 è¯¦ç»†"ï¼‰
        match = re.match(r'^(\d+(?:\.\d+)*)', title)
        if match:
            num_part = match.group(1)
            dots = num_part.count('.')
            if dots == 0:
                return 1  # ä¸»ç« èŠ‚ å¦‚ "1. xxx"
            elif dots == 1:
                return 2  # å­ç« èŠ‚ å¦‚ "1.1 xxx"
            else:
                return 3  # æ›´æ·±å±‚çº§
        return 1  # é»˜è®¤ä¸ºä¸»ç« èŠ‚
    
    @staticmethod
    def _get_css_styles() -> str:
        """è·å– CSS æ ·å¼"""
        return '''        @page {
            size: A4;
            margin: 2cm 1.5cm;
            @bottom-center {
                content: counter(page);
                font-size: 10pt;
                color: #666;
            }
        }
        
        @page :first {
            @bottom-center { content: none; }
        }
        
        * { box-sizing: border-box; }
        
        body {
            font-family: "PingFang SC", "Microsoft YaHei", "Hiragino Sans GB", 
                         -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
            max-width: 100%;
            margin: 0;
            padding: 20px 40px;
            line-height: 1.8;
            color: #2c3e50;
            font-size: 11pt;
            background: white;
        }
        
        /* å°é¢ */
        .cover {
            page-break-after: always;
            text-align: center;
            padding: 150px 20px 100px;
            min-height: 90vh;
        }
        
        .cover h1 {
            font-size: 36pt;
            color: #c0392b;
            margin-bottom: 20px;
            border: none;
            padding: 0;
        }
        
        .cover .logo {
            font-size: 80pt;
            margin: 40px 0;
        }
        
        .cover .source {
            font-size: 11pt;
            color: #95a5a6;
            margin-top: 60px;
        }
        
        /* ç›®å½• */
        .toc {
            page-break-after: always;
            padding: 20px;
        }
        
        .toc h1 {
            text-align: center;
            color: #2c3e50;
            border: none;
            margin-bottom: 20px;
        }
        
        .toc-table {
            width: 100%;
            border: none;
            border-collapse: collapse;
        }
        
        .toc-table td {
            width: 50%;
            vertical-align: top;
            padding: 0 10px;
            border: none;
        }
        
        .toc-item {
            margin: 3px 0;
            color: #34495e;
            line-height: 1.5;
            font-size: 9pt;
        }
        
        .toc-item.level-1 {
            font-weight: bold;
            margin-top: 10px;
            font-size: 10pt;
            color: #2c3e50;
        }
        
        .toc-item.level-2 {
            padding-left: 12px;
        }
        
        .toc-item.level-3 {
            padding-left: 24px;
            font-size: 8pt;
            color: #7f8c8d;
        }
        
        /* PDF ä¹¦ç­¾å±‚çº§è®¾ç½® */
        .chapter-title {
            string-set: chapter-title content();
        }
        
        h1.bookmark-1, .bookmark-1 {
            bookmark-level: 1;
            bookmark-state: open;
        }
        
        h2.bookmark-2, .bookmark-2 {
            bookmark-level: 2;
            bookmark-state: closed;
        }
        
        h3.bookmark-3, .bookmark-3 {
            bookmark-level: 3;
            bookmark-state: closed;
        }
        
        /* ç« èŠ‚æ ‡é¢˜æ ·å¼ */
        h1 {
            color: #c0392b;
            font-size: 22pt;
            border-bottom: 3px solid #e74c3c;
            padding-bottom: 10px;
            margin-top: 0;
            page-break-after: avoid;
            bookmark-level: 1;
        }
        
        h2 {
            color: #2980b9;
            font-size: 16pt;
            border-bottom: 2px solid #3498db;
            padding-bottom: 6px;
            margin-top: 1.5em;
            page-break-after: avoid;
            bookmark-level: 2;
        }
        
        h3 {
            color: #27ae60;
            font-size: 13pt;
            margin-top: 1.2em;
            page-break-after: avoid;
            bookmark-level: 3;
        }
        
        h4, h5, h6 {
            color: #8e44ad;
            margin-top: 1em;
        }
        
        /* ä»£ç æ ·å¼ */
        code {
            background-color: #f4f4f4;
            padding: 2px 5px;
            border-radius: 3px;
            font-family: "JetBrains Mono", "Fira Code", "SF Mono", Consolas, 
                         "Liberation Mono", Menlo, monospace;
            font-size: 9.5pt;
            color: #c7254e;
        }
        
        pre {
            background-color: #282c34;
            color: #abb2bf;
            padding: 14px;
            border-radius: 6px;
            overflow-x: auto;
            font-size: 9pt;
            line-height: 1.45;
            page-break-inside: avoid;
            white-space: pre-wrap;
            word-wrap: break-word;
            margin: 1em 0;
        }
        
        pre code {
            background-color: transparent;
            padding: 0;
            color: inherit;
            font-size: inherit;
        }
        
        /* ç« èŠ‚åˆ†éš” */
        .chapter {
            page-break-before: always;
        }
        
        .chapter:first-of-type {
            page-break-before: avoid;
        }
        
        /* é“¾æ¥ */
        a {
            color: #3498db;
            text-decoration: none;
        }
        
        /* å¼•ç”¨å— */
        blockquote {
            border-left: 4px solid #f39c12;
            margin: 1em 0;
            padding: 10px 20px;
            background-color: #fef9e7;
            color: #7d6608;
            page-break-inside: avoid;
        }
        
        /* è¡¨æ ¼ */
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 1em 0;
            page-break-inside: avoid;
            font-size: 10pt;
        }
        
        th, td {
            border: 1px solid #bdc3c7;
            padding: 8px 10px;
            text-align: left;
        }
        
        th {
            background-color: #3498db;
            color: white;
        }
        
        tr:nth-child(even) {
            background-color: #ecf0f1;
        }
        
        /* å›¾ç‰‡ */
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 1em auto;
        }
        
        /* åˆ—è¡¨ */
        ul, ol {
            padding-left: 25px;
        }
        
        li {
            margin: 4px 0;
        }
        
        /* éšè—ä¸éœ€è¦çš„å…ƒç´  */
        .buttons, .fa, .fa-play, .fa-copy,
        button, .play-button, .test-arrow,
        .header, .nav-chapters, .chapter-nav {
            display: none !important;
        }
        
        /* ç¦ç”¨å†…å®¹ä¸­æ ‡é¢˜çš„ä¹¦ç­¾ï¼ˆåªä¿ç•™ç« èŠ‚æ ‡é¢˜çš„ä¹¦ç­¾ï¼‰ */
        .no-bookmark {
            bookmark-level: none;
        }
        
        /* å°é¢å’Œç›®å½•æ ‡é¢˜ä¸ç”Ÿæˆä¹¦ç­¾ */
        .cover h1, .toc h1 {
            bookmark-level: none;
        }'''
    
    def _generate_toc_html(self) -> str:
        """ç”Ÿæˆç›®å½• HTML"""
        items = list(self.chapters.items())
        mid = (len(items) + 1) // 2
        
        toc_html = '        <td>\n'
        for url, title in items[:mid]:
            level = self._get_toc_level(title)
            toc_html += f'            <div class="toc-item level-{level}">{title}</div>\n'
        toc_html += '        </td>\n'
        
        toc_html += '        <td>\n'
        for url, title in items[mid:]:
            level = self._get_toc_level(title)
            toc_html += f'            <div class="toc-item level-{level}">{title}</div>\n'
        toc_html += '        </td>\n'
        
        return toc_html
    
    def _generate_chapters_html(self) -> str:
        """ç”Ÿæˆç« èŠ‚å†…å®¹ HTML"""
        chapters_html = ''
        for i, page in enumerate(self.pages):
            level = self._get_toc_level(page['title'])
            heading_tag = f'h{min(level, 3)}'
            
            chapters_html += f'''
<div class="chapter" id="chapter-{i}">
<{heading_tag} class="chapter-title bookmark-{level}">{page['title']}</{heading_tag}>
{page['content']}
</div>
'''
        return chapters_html
    
    def generate_html(self) -> str:
        """
        ç”Ÿæˆåˆå¹¶çš„ HTML æ–‡ä»¶
        
        Returns:
            å®Œæ•´çš„ HTML å†…å®¹
        """
        html = f'''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>{self.book_title}</title>
    <style>
{self._get_css_styles()}
    </style>
</head>
<body>

<!-- å°é¢ -->
<div class="cover">
    <div class="logo">ğŸ¦€</div>
    <h1>{self.book_title}</h1>
    <p class="source">
        æ¥æºï¼š<a href="{self.base_url}">{self.base_url}</a>
    </p>
</div>

<!-- ç›®å½• -->
<div class="toc">
    <h1>ç›® å½•</h1>
    <table class="toc-table"><tr>
{self._generate_toc_html()}    </tr></table>
</div>

{self._generate_chapters_html()}
</body>
</html>
'''
        
        return html
    
    @staticmethod
    def _sanitize_filename(title: str) -> str:
        """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
        return re.sub(FILENAME_UNSAFE_CHARS, '_', title)
    
    def save_html(self) -> str:
        """
        ä¿å­˜ HTML æ–‡ä»¶
        
        Returns:
            HTML æ–‡ä»¶è·¯å¾„
        """
        os.makedirs(self.output_dir, exist_ok=True)
        
        html_content = self.generate_html()
        safe_title = self._sanitize_filename(self.book_title)
        html_file = os.path.join(self.output_dir, f'{safe_title}.html')
        
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        print(f"âœ… HTML æ–‡ä»¶å·²ä¿å­˜: {html_file}")
        return html_file
    
    def convert_to_pdf(self, html_file: str) -> Optional[str]:
        """
        ä½¿ç”¨ weasyprint è½¬æ¢ä¸º PDF
        
        Args:
            html_file: HTML æ–‡ä»¶è·¯å¾„
            
        Returns:
            PDF æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            from weasyprint import HTML
            from weasyprint.text.fonts import FontConfiguration
        except ImportError:
            print("\nâŒ WeasyPrint æœªå®‰è£…")
            print("   è¯·è¿è¡Œ: pip install weasyprint")
            print("   macOS è¿˜éœ€è¦: brew install pango")
            return None
        
        try:
            print("\nğŸ“„ æ­£åœ¨ä½¿ç”¨ WeasyPrint ç”Ÿæˆ PDF...")
            print("   (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...)")
            
            font_config = FontConfiguration()
            safe_title = self._sanitize_filename(self.book_title)
            pdf_file = os.path.join(self.output_dir, f'{safe_title}.pdf')
            
            html = HTML(filename=html_file)
            html.write_pdf(pdf_file, font_config=font_config)
            
            # è·å–æ–‡ä»¶å¤§å°
            size_mb = os.path.getsize(pdf_file) / (1024 * 1024)
            print("\nâœ… PDF ç”ŸæˆæˆåŠŸ!")
            print(f"   æ–‡ä»¶: {pdf_file}")
            print(f"   å¤§å°: {size_mb:.1f} MB")
            return pdf_file
            
        except Exception as e:
            print(f"\nâŒ PDF ç”Ÿæˆå¤±è´¥: {e}")
            return None


def _print_header():
    """æ‰“å°ç¨‹åºæ ‡é¢˜"""
    print("=" * 60)
    print("  ğŸ“š mdBook é€šç”¨çˆ¬è™« - PDF ç”Ÿæˆå™¨")
    print("=" * 60)


def _print_success():
    """æ‰“å°æˆåŠŸä¿¡æ¯"""
    print("\n" + "=" * 60)
    print("  ğŸ‰ å®Œæˆï¼")
    print("=" * 60)


def _print_html_only_success():
    """æ‰“å°ä»… HTML ç”ŸæˆæˆåŠŸä¿¡æ¯"""
    print("\n" + "=" * 60)
    print("  ğŸ‰ HTML ç”Ÿæˆå®Œæˆï¼")
    print("=" * 60)


def _print_fallback_options(html_file: str):
    """æ‰“å°å¤‡é€‰æ–¹æ¡ˆ"""
    print("\n" + "-" * 60)
    print("å¤‡é€‰æ–¹æ¡ˆ:")
    print(f"  1. ç”¨æµè§ˆå™¨æ‰“å¼€ {html_file}")
    print("  2. æŒ‰ Ctrl+P (Mac: Cmd+P) æ‰“å°ä¸º PDF")
    print("-" * 60)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='é€šç”¨ mdBook ç½‘ç«™çˆ¬è™« - è‡ªåŠ¨çˆ¬å–å¹¶ç”Ÿæˆ PDF',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹:
  python mbook2pdf.py https://rustwiki.org/zh-CN/book/
  python mbook2pdf.py https://rustwiki.org/zh-CN/rust-by-example/
  python mbook2pdf.py https://colobu.com/rust100/
  python mbook2pdf.py https://doc.rust-lang.org/book/ -o ./rust_book
        '''
    )
    
    parser.add_argument('url', help='mdBook ç½‘ç«™ URL')
    parser.add_argument('-o', '--output', help='è¾“å‡ºç›®å½• (é»˜è®¤: æ ¹æ® URL è‡ªåŠ¨ç”Ÿæˆ)')
    parser.add_argument('-d', '--delay', type=float, default=DEFAULT_DELAY, 
                        help=f'è¯·æ±‚é—´éš”ç§’æ•° (é»˜è®¤: {DEFAULT_DELAY})')
    parser.add_argument('--html-only', action='store_true',
                        help='åªç”Ÿæˆ HTMLï¼Œä¸è½¬æ¢ PDF')
    
    args = parser.parse_args()
    
    _print_header()
    
    crawler = MdBookCrawler(args.url, args.output, args.delay)
    
    if not crawler.crawl():
        sys.exit(1)
    
    print("\nğŸ“ æ­£åœ¨ç”Ÿæˆ HTML æ–‡ä»¶...")
    html_file = crawler.save_html()
    
    if not args.html_only:
        pdf_file = crawler.convert_to_pdf(html_file)
        
        if pdf_file:
            _print_success()
        else:
            _print_fallback_options(html_file)
    else:
        _print_html_only_success()


if __name__ == "__main__":
    main()